{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting zip file to temp directory: ../lacmta/current-base/gtfs_bus.zip\n",
      "Extracting zip file to temp directory: ../lacmta-rail/current/gtfs_rail.zip\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import zipfile \n",
    "from shapely.geometry import Point\n",
    "# extract the gtfs files\n",
    "# unzip the gtfs files\n",
    "bus_zipfile = \"../lacmta/current-base/gtfs_bus.zip\"\n",
    "rail_zipfile = \"../lacmta-rail/current/gtfs_rail.zip\"\n",
    "\n",
    "def extract_gtfs_files():\n",
    "    print(\"Extracting GTFS files...\")\n",
    "    extract_start = timeit.default_timer()\n",
    "    with zipfile.ZipFile(bus_zipfile, 'r') as zipObj:\n",
    "        zipObj.extractall()\n",
    "    with zipfile.ZipFile(rail_zipfile, 'r') as zipObj:\n",
    "        zipObj.extractall()\n",
    "    extract_end = timeit.default_timer()\n",
    "    print(\"Extracting GTFS files took \" + str(extract_end - extract_start) + \" seconds.\")\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_zip_file_to_temp_directory(agency_id):\n",
    "    zip_file = None\n",
    "    if agency_id == 'lacmta':\n",
    "        zip_file = '../lacmta/current-base/gtfs_bus.zip'\n",
    "    elif agency_id == 'lacmta-rail':\n",
    "        zip_file = '../lacmta-rail/current/gtfs_rail.zip'\n",
    "    try:\n",
    "        print('Extracting zip file to temp directory: ' + zip_file)\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall('../temp/'+agency_id)\n",
    "    except Exception as e:\n",
    "        print('Error extracting zip file to temp directory: ' + str(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "extract_zip_file_to_temp_directory('lacmta')\n",
    "extract_zip_file_to_temp_directory('lacmta-rail')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************\n",
      "Starting with routes\n",
      "******************\n",
      "Starting with trips\n",
      "******************\n",
      "Starting with stops\n",
      "******************\n",
      "Starting with calendar\n",
      "******************\n",
      "Starting with shapes\n",
      "******************\n",
      "Starting with stop_times\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etria\\AppData\\Local\\Temp\\ipykernel_9912\\1134030024.py:102: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  temp_df_bus = pd.read_csv(bus_file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_df_rail\n",
      "    trip_id arrival_time departure_time  stop_id  stop_sequence  \\\n",
      "0  58503960     05:07:00       05:07:00    80101              1   \n",
      "1  58503960     05:09:00       05:09:00    80102              2   \n",
      "2  58503960     05:13:00       05:13:00    80105              3   \n",
      "3  58503960     05:16:00       05:16:00    80106              4   \n",
      "4  58503960     05:20:00       05:20:00    80107              5   \n",
      "\n",
      "                                 stop_headsign  pickup_type  drop_off_type  \\\n",
      "0  Metro A-Line - APU / Citrus College Station            0              0   \n",
      "1  Metro A-Line - APU / Citrus College Station            0              0   \n",
      "2  Metro A-Line - APU / Citrus College Station            0              0   \n",
      "3  Metro A-Line - APU / Citrus College Station            0              0   \n",
      "4  Metro A-Line - APU / Citrus College Station            0              0   \n",
      "\n",
      "     route_code              destination_code  timepoint    agency_id  \n",
      "0  Metro A-Line  APU / Citrus College Station          1  LACMTA_Rail  \n",
      "1  Metro A-Line  APU / Citrus College Station          1  LACMTA_Rail  \n",
      "2  Metro A-Line  APU / Citrus College Station          1  LACMTA_Rail  \n",
      "3  Metro A-Line  APU / Citrus College Station          1  LACMTA_Rail  \n",
      "4  Metro A-Line  APU / Citrus College Station          1  LACMTA_Rail  \n",
      "Processing trip list\n",
      "Creating list of trips\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etria\\AppData\\Local\\Temp\\ipykernel_9912\\1134030024.py:138: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  summarized_trips_df['day_type'] = summarized_trips_df['service_id'].map(get_day_type_from_service_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing trip_shape_stops_df ...\n",
      "Processing unique shape stop times...\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 56.1 GiB for an array with shape (7534677903,) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 188\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Write the result DataFrame to a new table in the database\u001b[39;00m\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone processing unique shape stop times.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 188\u001b[0m \u001b[43mupdate_gtfs_static_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 159\u001b[0m, in \u001b[0;36mupdate_gtfs_static_files\u001b[1;34m()\u001b[0m\n\u001b[0;32m    157\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(trip_shape_stops_df, trips_df, on\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrip_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    158\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, route_stops_geo_data_frame, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 159\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshapes_combined_gdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mshape_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;66;03m# Get unique route_codes\u001b[39;00m\n\u001b[0;32m    162\u001b[0m unique_route_codes \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute_code\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\n",
      "File \u001b[1;32mc:\\Users\\etria\\miniconda3\\envs\\metro\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:183\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    170\u001b[0m         left_df,\n\u001b[0;32m    171\u001b[0m         right_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\etria\\miniconda3\\envs\\metro\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:883\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 883\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    885\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_and_concat(\n\u001b[0;32m    886\u001b[0m     join_index, left_indexer, right_indexer, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    887\u001b[0m )\n\u001b[0;32m    888\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_type)\n",
      "File \u001b[1;32mc:\\Users\\etria\\miniconda3\\envs\\metro\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1133\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m   1130\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m   1131\u001b[0m     )\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\etria\\miniconda3\\envs\\metro\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1105\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_join_indexers\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\etria\\miniconda3\\envs\\metro\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1728\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how)\u001b[0m\n\u001b[0;32m   1718\u001b[0m join_func \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39minner_join,\n\u001b[0;32m   1720\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mleft_outer_join,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1724\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mouter\u001b[39m\u001b[38;5;124m\"\u001b[39m: libjoin\u001b[38;5;241m.\u001b[39mfull_outer_join,\n\u001b[0;32m   1725\u001b[0m }[how]\n\u001b[0;32m   1727\u001b[0m \u001b[38;5;66;03m# error: Cannot call function of unknown type\u001b[39;00m\n\u001b[1;32m-> 1728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjoin_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcount\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mjoin.pyx:48\u001b[0m, in \u001b[0;36mpandas._libs.join.inner_join\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 56.1 GiB for an array with shape (7534677903,) and data type int64"
     ]
    }
   ],
   "source": [
    "list_of_gtfs_static_files = [\"routes\", \"trips\", \"stops\", \"calendar\", \"shapes\",\"stop_times\"]\n",
    "debug = True\n",
    "\n",
    "\n",
    "def update_stops_seperately(temp_df_bus,temp_df_rail,file):\n",
    "    temp_df_bus['agency_id'] = 'LACMTA'\n",
    "    temp_gdf_bus_stops = gpd.GeoDataFrame(temp_df_bus,geometry=gpd.points_from_xy(temp_df_bus.stop_lon, temp_df_bus.stop_lat))\n",
    "    temp_gdf_bus_stops.set_crs(epsg=4326, inplace=True)\n",
    "    temp_df_rail['agency_id'] = 'LACMTA_Rail'\n",
    "    temp_gdf_bus_stops['stop_id'] = temp_gdf_bus_stops['stop_id'].astype('str')\n",
    "    temp_gdf_bus_stops['stop_code'] = temp_gdf_bus_stops['stop_code'].astype('str')\n",
    "    temp_gdf_bus_stops['parent_station'] = temp_gdf_bus_stops['parent_station'].astype('str')\n",
    "    temp_gdf_bus_stops['tpis_name'] = temp_gdf_bus_stops['tpis_name'].astype('str')\n",
    "\n",
    "    temp_gdf_rail_stops = gpd.GeoDataFrame(temp_df_rail,geometry=gpd.points_from_xy(temp_df_rail.stop_lon, temp_df_rail.stop_lat))\n",
    "    temp_gdf_rail_stops.set_crs(epsg=4326, inplace=True)\n",
    "    temp_gdf_rail_stops['stop_id'] = temp_gdf_rail_stops['stop_id'].astype('str')\n",
    "    temp_gdf_rail_stops['stop_code'] = temp_gdf_rail_stops['stop_code'].astype('str')\n",
    "    temp_gdf_rail_stops['parent_station'] = temp_gdf_rail_stops['parent_station'].astype('str')\n",
    "    temp_gdf_rail_stops['tpis_name'] = temp_gdf_rail_stops['tpis_name'].astype('str')\n",
    "    return pd.concat([temp_gdf_bus_stops,temp_gdf_rail_stops])\n",
    "\n",
    "def create_gdf_for_shapes(temp_df_bus,temp_df_rail):\n",
    "    temp_gdf_bus = gpd.GeoDataFrame(temp_df_bus, geometry=gpd.points_from_xy(temp_df_bus.shape_pt_lon, temp_df_bus.shape_pt_lat))   \n",
    "    temp_gdf_rail = gpd.GeoDataFrame(temp_df_rail, geometry=gpd.points_from_xy(temp_df_rail.shape_pt_lon, temp_df_rail.shape_pt_lat))\n",
    "    shapes_combined_gdf = gpd.GeoDataFrame(pd.concat([temp_gdf_bus, temp_gdf_rail],ignore_index=True),geometry='geometry')\n",
    "    shapes_combined_gdf.crs = 'EPSG:4326'\n",
    "    return shapes_combined_gdf\n",
    "def get_stop_times_for_trip_id(this_row):\n",
    "    this_trips_df = stop_times_df.loc[stop_times_df['trip_id'] == this_row.trip_id]\n",
    "    this_trips_df['route_id'] = this_row.route_id\n",
    "    this_trips_df['direction_id'] = this_row.direction_id\n",
    "    this_trips_df['day_type'] = this_row.day_type\n",
    "    this_trips_df['geojson'] = this_trips_df.apply(lambda x: get_stops_data_based_on_stop_id(x.stop_id),axis=1)\n",
    "    this_trips_df['stop_name'] = this_trips_df.apply(lambda x: stops_df.loc[stops_df['stop_id'] == str(x.stop_id)]['stop_name'].values[0],axis=1)\n",
    "    simplified_df = this_trips_df[['route_id','route_code','stop_id','day_type','stop_sequence','direction_id','stop_name','geojson','agency_id']]\n",
    "    \n",
    "    df_to_combine.append(simplified_df)\n",
    "    return simplified_df\n",
    "def encode_lat_lon_to_geojson(lat,lon):\n",
    "    this_geojson = {\n",
    "        \"type\":\"Feature\",\n",
    "        \"geometry\":{\n",
    "            \"type\":\"Point\",\n",
    "            \"coordinates\": [lon,lat]\n",
    "        }\n",
    "    }\n",
    "    return this_geojson\n",
    "\n",
    "def get_stops_data_based_on_stop_id(stop_id):\n",
    "    this_stops_df = stops_df.loc[stops_df['stop_id'] == str(stop_id)]\n",
    "    new_object = encode_lat_lon_to_geojson(this_stops_df['stop_lat'].values[0],this_stops_df['stop_lon'].values[0])\n",
    "    return new_object\n",
    "\n",
    "df_to_combine = []\n",
    "\n",
    "def create_list_of_trips(trips,stop_times):\n",
    "    print('Creating list of trips')\n",
    "    global trips_list_df\n",
    "    trips_list_df = stop_times.groupby('trip_id')['stop_sequence'].max().sort_values(ascending=False).reset_index()\n",
    "    trips_list_df = trips_list_df.merge(stop_times[['trip_id','stop_id','stop_sequence','route_code']], on=['trip_id','stop_sequence'])\n",
    "    return trips_list_df\n",
    "\n",
    "\n",
    "\n",
    "def get_stop_times_from_stop_id(this_row):\n",
    "    trips_by_route_df = trips_df.loc[trips_df['route_id'] == this_row.route_id]\n",
    "    stop_times_by_trip_df = stop_times_df[stop_times_df['trip_id'].isin(trips_by_route_df['trip_id'])]\n",
    "\n",
    "    # get the stop times for this stop id\n",
    "    this_stops_df = stop_times_by_trip_df.loc[stop_times_by_trip_df['stop_id'] == this_row.stop_id]\n",
    "    this_stops_df = this_stops_df.sort_values(by=['departure_time'],ascending=True)\n",
    "    departure_times_array = this_stops_df['departure_time'].values.tolist()\n",
    "    return departure_times_array\n",
    "\n",
    "def get_day_type_from_service_id(row):\n",
    "    # print('Getting day type from service id')\n",
    "    cleaned_row = str(row).lower()\n",
    "    if 'weekday' in cleaned_row:\n",
    "        return 'weekday'\n",
    "    elif 'saturday' in cleaned_row:\n",
    "        return 'saturday'\n",
    "    elif 'sunday' in cleaned_row:\n",
    "        return 'sunday'\n",
    "def get_lat_long_from_coordinates(geojson):\n",
    "    this_geojson_geom = geojson['geometry']\n",
    "    return Point(this_geojson_geom['coordinates'][0], this_geojson_geom['coordinates'][1])    \n",
    "def update_gtfs_static_files():\n",
    "    global stop_times_df\n",
    "    global trips_df\n",
    "    global calendar_dates_df\n",
    "    global calendar_df\n",
    "    global stops_df\n",
    "    for file in list_of_gtfs_static_files:\n",
    "        print(\"******************\")\n",
    "        print(\"Starting with \" + file)\n",
    "        bus_file_path = \"\"\n",
    "        rail_file_path = \"\"\n",
    "\n",
    "        bus_file_path = \"../temp/lacmta/\" + file + '.txt'\n",
    "        rail_file_path = \"../temp/lacmta-rail/\" + file + '.txt'\n",
    "        temp_df_bus = pd.read_csv(bus_file_path)\n",
    "        temp_df_bus['agency_id'] = 'LACMTA'\n",
    "        temp_df_rail = pd.read_csv(rail_file_path)\n",
    "        temp_df_rail['agency_id'] = 'LACMTA_Rail'\n",
    "        if file == \"stops\":\n",
    "            stops_df = update_stops_seperately(temp_df_bus,temp_df_rail,file)\n",
    "        elif file == \"shapes\":\n",
    "            shapes_combined_gdf = create_gdf_for_shapes(temp_df_bus,temp_df_rail)\n",
    "        elif file == \"stop_times\":            \n",
    "            print('temp_df_rail')\n",
    "            print(temp_df_rail.head())\n",
    "            temp_df_rail['trip_id'] = temp_df_rail['trip_id'].astype(str)\n",
    "            temp_df_bus['trip_id'] = temp_df_bus['trip_id'].astype(str)\n",
    "            cols = ['pickup_type','drop_off_type']\n",
    "            combined_temp_df = pd.concat([temp_df_bus, temp_df_rail])\n",
    "            combined_temp_df['rider_usage_code_before_coding'] = combined_temp_df[cols].apply(lambda row: ''.join(row.values.astype(str)), axis=1)\n",
    "            combined_temp_df['rider_usage_code'] = combined_temp_df['rider_usage_code_before_coding'].apply(lambda x: 1 if x == '00' else 2 if x == '10' else 3 if x == '01' else 0 if x == '11' else -1)\n",
    "            if 'bay_num' not in combined_temp_df.columns:\n",
    "                combined_temp_df['bay_num'] = -1\n",
    "            combined_temp_df.drop(columns=['rider_usage_code_before_coding'])\n",
    "            stop_times_df = combined_temp_df\n",
    "\n",
    "        else:\n",
    "            combined_temp_df = pd.concat([temp_df_bus, temp_df_rail])\n",
    "\n",
    "            if file == \"trips\":\n",
    "                trips_df = combined_temp_df\n",
    "            if file == \"calendar_dates\":\n",
    "                calendar_dates_df = combined_temp_df\n",
    "            if file == \"calendar\":\n",
    "                calendar_df = combined_temp_df\n",
    "\n",
    "                # combined_temp_df.to_sql(file,engine,index=False,if_exists=\"replace\",schema=TARGET_SCHEMA)\n",
    "    print(\"Processing trip list\")\n",
    "    trips_list_df = create_list_of_trips(trips_df,stop_times_df)\n",
    "    summarized_trips_df = trips_df[[\"route_id\",\"trip_id\",\"direction_id\",\"service_id\",\"agency_id\"]]\n",
    "    summarized_trips_df['day_type'] = summarized_trips_df['service_id'].map(get_day_type_from_service_id)\n",
    "    trips_list_df = trips_list_df.merge(summarized_trips_df, on='trip_id').drop_duplicates(subset=['route_id','day_type','direction_id'])\n",
    "    trips_list_df.apply(lambda row: get_stop_times_for_trip_id(row), axis=1)\n",
    "    stop_times_by_route_df = pd.concat(df_to_combine)\n",
    "    stop_times_by_route_df['departure_times'] = stop_times_by_route_df.apply(lambda row: get_stop_times_from_stop_id(row),axis=1)\n",
    "    stop_times_by_route_df['route_code'].fillna(stop_times_by_route_df['route_id'], inplace=True)\n",
    "    route_stops_geo_data_frame = gpd.GeoDataFrame(stop_times_by_route_df, geometry=stop_times_by_route_df.apply(lambda x: get_lat_long_from_coordinates(x.geojson),axis=1))\n",
    "    route_stops_geo_data_frame.set_crs(epsg=4326, inplace=True)\n",
    "    # Perform the joins\n",
    "    print(\"Processing trip_shape_stops_df ...\")\n",
    "    # Merge stop_times_df with trips_df on 'trip_id'\n",
    "    merged_df = pd.merge(stop_times_df, trips_df, on='trip_id')\n",
    "\n",
    "    # Create trip_shape_stops_df\n",
    "    trip_shape_stops_df = merged_df.groupby(['trip_id', 'shape_id'])['stop_id'].apply(list).reset_index()\n",
    "    print(\"Processing unique shape stop times...\")\n",
    "    \n",
    "\n",
    "    # Perform the joins\n",
    "    df = pd.merge(trip_shape_stops_df, trips_df, on=['trip_id', 'shape_id'])\n",
    "    df = pd.merge(df, route_stops_geo_data_frame, on='route_id')\n",
    "    df = pd.merge(df, shapes_combined_gdf, on='shape_id')\n",
    "\n",
    "    # Get unique route_codes\n",
    "    unique_route_codes = df['route_code'].unique()\n",
    "\n",
    "    # Create an empty DataFrame to store the results\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Process each unique route_code\n",
    "    for route_code in unique_route_codes:\n",
    "        df_route_code = df[df['route_code'] == route_code]\n",
    "\n",
    "        # Get unique direction_ids for the current route_code\n",
    "        unique_direction_ids = df_route_code['direction_id'].unique()\n",
    "\n",
    "        # Process each unique direction_id\n",
    "        for direction_id in unique_direction_ids:\n",
    "            df_route = df_route_code[df_route_code['direction_id'] == direction_id]\n",
    "\n",
    "            # Sort the DataFrame\n",
    "            df_route = df_route.sort_values(['service_id', 'trip_id', 'stop_sequence'])\n",
    "\n",
    "            # Append the sorted DataFrame to the result DataFrame\n",
    "            result_df = result_df.append(df_route)\n",
    "\n",
    "    # Write the result DataFrame to a new table in the database\n",
    "    print(\"Done processing unique shape stop times.\")\n",
    "\n",
    "\n",
    "update_gtfs_static_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming stop_times_df, trips_df, route_stops_geo_data_frame, shapes_combined_gdf are already defined\n",
    "\n",
    "# Select only the necessary columns from stop_times\n",
    "stop_times_df = stop_times_df[['trip_id', 'stop_id', 'arrival_time']]\n",
    "\n",
    "# Aggregate the data if necessary\n",
    "# For example, get the first and last arrival time for each trip\n",
    "stop_times_df = stop_times_df.groupby('trip_id').agg({'arrival_time': ['min', 'max']}).reset_index()\n",
    "\n",
    "# Create trip_shape_stops_df\n",
    "trip_shape_stops_df = merged_df.groupby(['trip_id', 'shape_id'])['stop_id'].apply(list).reset_index()\n",
    "print(\"Processing unique shape stop times...\")\n",
    "\n",
    "# Perform the joins\n",
    "df = pd.merge(trip_shape_stops_df, stop_times_df, on='trip_id')\n",
    "df = pd.merge(df, trips_df, on='trip_id')\n",
    "df = pd.merge(df, route_stops_geo_data_frame, on='route_id')\n",
    "df = pd.merge(df, shapes_combined_gdf, on='shape_id')\n",
    "\n",
    "# Get unique route_codes\n",
    "unique_route_codes = df['route_code'].unique()\n",
    "\n",
    "# Create an empty DataFrame to store the results\n",
    "result_df = pd.DataFrame()\n",
    "\n",
    "# Process each unique route_code\n",
    "for route_code in unique_route_codes:\n",
    "    df_route_code = df[df['route_code'] == route_code]\n",
    "\n",
    "    # Get unique direction_ids for the current route_code\n",
    "    unique_direction_ids = df_route_code['direction_id'].unique()\n",
    "\n",
    "    # Process each unique direction_id\n",
    "    for direction_id in unique_direction_ids:\n",
    "        df_route = df_route_code[df_route_code['direction_id'] == direction_id]\n",
    "\n",
    "        # Sort the DataFrame\n",
    "        df_route = df_route.sort_values(['service_id', 'trip_id', 'stop_sequence'])\n",
    "\n",
    "        # Append the sorted DataFrame to the result DataFrame\n",
    "        result_df = result_df.append(df_route)\n",
    "\n",
    "# Write the result DataFrame to a new table in the database\n",
    "print(\"Done processing unique shape stop times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the joins\n",
    "df = pd.merge(trip_shape_stops_df, trips_df[['trip_id', 'direction_id', 'service_id']], on='trip_id')\n",
    "df = pd.merge(df, stop_times_df[['trip_id', 'stop_id', 'departure_time']], on=['trip_id', 'stop_id'])\n",
    "\n",
    "# Assuming calendar_df is your DataFrame containing 'service_id' and 'day_of_week'\n",
    "df = pd.merge(df, calendar_df, on='service_id')\n",
    "\n",
    "# Group by 'direction_id', 'day_of_week', 'trip_id', 'stop_id' and aggregate 'departure_time'\n",
    "df = df.groupby(['direction_id', 'day_of_week', 'trip_id', 'stop_id'])['departure_time'].apply(list).reset_index()\n",
    "\n",
    "# Pivot the DataFrame to get the desired hierarchy\n",
    "df = df.pivot_table(index=['direction_id', 'day_of_week', 'trip_id', 'stop_id'], values='departure_time', aggfunc='first').reset_index()\n",
    "\n",
    "# Convert the DataFrame to a dictionary for easier viewing of the hierarchy\n",
    "data_dict = df.to_dict('records')\n",
    "\n",
    "# Write the result DataFrame to a new table in the database\n",
    "print(\"Done processing unique shape stop times.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge stop_times_df with trips_df on 'trip_id'\n",
    "merged_df = pd.merge(stop_times_df, trips_df, on='trip_id')\n",
    "\n",
    "# Create trip_shape_stops_df\n",
    "trip_shape_stops_df = merged_df.groupby(['trip_id', 'shape_id'])['stop_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge stop_times_df with trips_df on 'trip_id'\n",
    "merged_df = pd.merge(stop_times_df, trips_df, on='trip_id')\n",
    "\n",
    "# Create trip_shape_stops_df\n",
    "trip_shape_stops_df = merged_df.groupby(['trip_id', 'shape_id'])['stop_id'].apply(list).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'route_stops_geo_data_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(stop_times_df, trips_df, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrip_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Continue with the rest of your code...\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, \u001b[43mroute_stops_geo_data_frame\u001b[49m[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute_id\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroute_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(df, shapes_combined_gdf[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape_id\u001b[39m\u001b[38;5;124m'\u001b[39m]], on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sort values before merge\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'route_stops_geo_data_frame' is not defined"
     ]
    }
   ],
   "source": [
    "# Select only necessary columns\n",
    "stop_times_df = stop_times_df[['trip_id', 'route_code', 'stop_sequence']]\n",
    "trips_df = trips_df[['trip_id', 'route_id', 'shape_id']]\n",
    "\n",
    "# Perform merge operation\n",
    "df = pd.merge(stop_times_df, trips_df, on='trip_id')\n",
    "\n",
    "# Continue with the rest of your code...\n",
    "df = pd.merge(df, route_stops_geo_data_frame[['route_id']], on='route_id')\n",
    "df = pd.merge(df, shapes_combined_gdf[['shape_id']], on='shape_id')\n",
    "\n",
    "# Sort values before merge\n",
    "df = df.sort_values(['trip_id', 'stop_sequence'])\n",
    "\n",
    "# Use groupby instead of looping\n",
    "grouped = df.groupby(['route_code'])\n",
    "\n",
    "# Use concat instead of append\n",
    "result_df = pd.concat([group.sort_values(['trip_id', 'stop_sequence']) for name, group in grouped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'direction_id' and 'service_id' exist in stop_times_df\n",
    "if 'direction_id' in stop_times_df.columns and 'service_id' in stop_times_df.columns:\n",
    "    df = pd.merge(stop_times_df[['trip_id', 'route_code', 'direction_id', 'service_id', 'stop_sequence']], \n",
    "                trips_df[['trip_id']], on='trip_id')\n",
    "else:\n",
    "    print(\"Columns 'direction_id' and/or 'service_id' do not exist in stop_times_df\")\n",
    "    df = pd.merge(stop_times_df[['trip_id', 'route_code', 'stop_sequence']], \n",
    "                trips_df[['trip_id']], on='trip_id')\n",
    "\n",
    "df = pd.merge(df, route_stops_geo_data_frame[['route_id']], on='route_id')\n",
    "df = pd.merge(df, shapes_combined_gdf[['shape_id']], on='shape_id')\n",
    "\n",
    "# Sort values before merge\n",
    "df = df.sort_values(['trip_id', 'stop_sequence'])\n",
    "\n",
    "# Use groupby instead of looping\n",
    "grouped = df.groupby(['route_code'])\n",
    "\n",
    "# Use concat instead of append\n",
    "result_df = pd.concat([group.sort_values(['trip_id', 'stop_sequence']) for name, group in grouped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge only necessary columns\n",
    "df = pd.merge(stop_times_df[['trip_id', 'route_code', 'direction_id', 'service_id', 'stop_sequence']], \n",
    "              trips_df[['trip_id']], on='trip_id')\n",
    "df = pd.merge(df, route_stops_geo_data_frame[['route_id']], on='route_id')\n",
    "df = pd.merge(df, shapes_combined_gdf[['shape_id']], on='shape_id')\n",
    "\n",
    "# Sort values before merge\n",
    "df = df.sort_values(['service_id', 'trip_id', 'stop_sequence'])\n",
    "\n",
    "# Use groupby instead of looping\n",
    "grouped = df.groupby(['route_code', 'direction_id'])\n",
    "\n",
    "# Use concat instead of append\n",
    "result_df = pd.concat([group.sort_values(['service_id', 'trip_id', 'stop_sequence']) for name, group in grouped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge only necessary columns\n",
    "df = pd.merge(stop_times_df[['trip_id', 'route_code', 'direction_id', 'service_id', 'stop_sequence']], \n",
    "              trips_df[['trip_id']], on='trip_id')\n",
    "df = pd.merge(df, route_stops_geo_data_frame[['route_id']], on='route_id')\n",
    "df = pd.merge(df, shapes_combined_gdf[['shape_id']], on='shape_id')\n",
    "\n",
    "# Sort values before merge\n",
    "df = df.sort_values(['service_id', 'trip_id', 'stop_sequence'])\n",
    "\n",
    "# Use groupby instead of looping\n",
    "grouped = df.groupby(['route_code', 'direction_id'])\n",
    "\n",
    "# Use concat instead of append\n",
    "result_df = pd.concat([group.sort_values(['service_id', 'trip_id', 'stop_sequence']) for name, group in grouped])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrotests",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
